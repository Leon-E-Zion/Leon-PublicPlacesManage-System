{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6f72d0",
   "metadata": {},
   "source": [
    "# 实现 任务参数 记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fde146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30703fd5",
   "metadata": {},
   "source": [
    "# 图像源 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962188bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    " \n",
    "class VideoCamera(object):\n",
    "    def __init__(self):\n",
    "        # 定义视频数据的图像来源\n",
    "        self.video = cv2.VideoCapture(0)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.video.release()\n",
    "    \n",
    "    # 设置获取视频的图像数据\n",
    "    def get_frame(self):\n",
    "        # 每次运行都通过来源进行数据获取\n",
    "        success, image = self.video.read()\n",
    "        # 将图片进行编码，转化为指定的图片格式\n",
    "        ret, jpeg = cv2.imencode('.jpg', image)\n",
    "        # 通过字节形式进行流传输\n",
    "        return jpeg.tobytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8bf90",
   "metadata": {},
   "source": [
    "# 相关功能函数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa057011",
   "metadata": {},
   "source": [
    "### 图像标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ba4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# cv2-图像 [x1,y1,x2,y2]-坐标框 贴图文本\n",
    "def draw_img(img,bbox,text):\n",
    "    image = img.copy()\n",
    "    # 左上角\n",
    "    start_point = (int(bbox[1]),int(bbox[0]))\n",
    "    # 右下角\n",
    "    end_point = (int(bbox[3]),int(bbox[2]))\n",
    "    # draw the rectangle\n",
    "    cv2.rectangle(image, start_point, end_point, (0, 0, 255), thickness= 3, lineType=cv2.LINE_8) \n",
    "    #org: Where you want to put the text\n",
    "    org = start_point\n",
    "    # write the text on the input image\n",
    "    cv2.putText(image, text, org, fontFace = cv2.FONT_HERSHEY_COMPLEX, fontScale = 1.8, color = (0,0,250))\n",
    "    # display the output\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a634f0",
   "metadata": {},
   "source": [
    "## 行人检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21eceb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tools/leon_yolo.pth model, and classes loaded.\n",
      "Configurations:\n",
      "----------------------------------------------------------------------\n",
      "|                     keys |                                   values|\n",
      "----------------------------------------------------------------------\n",
      "|               model_path |                      tools/leon_yolo.pth|\n",
      "|             classes_path |                            tools/cls.txt|\n",
      "|              input_shape |                               [960, 960]|\n",
      "|                      phi |                                Leon_nano|\n",
      "|               confidence |                                      0.5|\n",
      "|                  nms_iou |                                      0.3|\n",
      "|          letterbox_image |                                     True|\n",
      "|                     cuda |                                     True|\n",
      "----------------------------------------------------------------------\n",
      "\t- img value_type:PIL.JpegImagePlugin.JpegImageFile\n",
      "\t+ crop\n",
      "\t|   + 0\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 1\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 2\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 3\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 4\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 5\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 6\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 7\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 8\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 9\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 10\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 11\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 12\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 13\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 14\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 15\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 16\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 17\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 18\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 19\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 20\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 21\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 22\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 23\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t- fea value_type:torch.Tensor\n",
      "\t- outputs value_type:list\n",
      "\t- img_orig value_type:list\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Oct  1 13:22:46 2022\n",
    "\n",
    "@author: leonzion\n",
    "\"\"\"\n",
    "\n",
    "from tools.yolo_leon_person import *\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from torchvision.ops import nms, roi_align, roi_pool\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "if sys.platform == 'win32':\n",
    "    sys.path.append(r'D:\\Leon-Coding\\Leon_FC')\n",
    "else:\n",
    "    sys.path.append(r'/home/leonzion/Leon_Coding/Leon_FC')\n",
    "from leon_info import *\n",
    "from leon_os import *\n",
    "# 初始化检测器\n",
    "yo = Yolo_infer_person()\n",
    "img = Image.open(r'D:\\Leon-Coding\\Leon_TestData\\mans_2.jpg')\n",
    "yolo_output = yo.infer(img)\n",
    "print(get_dict_dirs(yolo_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e19fc",
   "metadata": {},
   "source": [
    "## 指定人物追踪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f7e16",
   "metadata": {},
   "source": [
    "### 逐一获取行人数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e11ab6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yolo_mes = yolo_output['crop']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfe0d9",
   "metadata": {},
   "source": [
    "### 重识别的函数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61d44a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Oct  2 12:58:46 2022\n",
    "\n",
    "@author: leonzion\n",
    "\"\"\"\n",
    "import sys\n",
    "if sys.platform == 'win32':\n",
    "    sys.path.append(r'D:\\Leon-Coding\\Leon_FC')\n",
    "else:\n",
    "    sys.path.append(r'/home/leonzion/Leon_Coding/Leon_FC')\n",
    "from leon_info import *\n",
    "from leon_os import *\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "leon_path_in(os.path.join('reid','Person_reID_baseline_pytorch-master','model','ft_ResNet50'))\n",
    "leon_path_in(os.path.join('reid','Person_reID_baseline_pytorch-master'))\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((256,128), interpolation=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "data_transforms_infer = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256,128), interpolation=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 基于图片获取特征\n",
    "def get_fea(path):\n",
    "    img = data_transforms(Image.open(path))\n",
    "#     img = torch.tensor(img.transpose(2, 0, 1))\n",
    "    img = torch.unsqueeze(img, dim = 0)\n",
    "    img = torch.tensor(img).to(torch.float32)\n",
    "    net = torch.load('D:/Leon-Coding/Leon-PublicPlacesManage-System/models/reid-effi_b0.pth_net')\n",
    "    outputs = net(img) \n",
    "    # ---- L2-norm Feature ------\n",
    "    ff = outputs.data.cpu()\n",
    "    fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "    ff = ff.div(fnorm.expand_as(ff))\n",
    "    return ff\n",
    "\n",
    "def get_reidnet(net_path):\n",
    "    # 'D:/Leon-Coding/Leon-PublicPlacesManage-System/models/reid-effi_b0.pth_net'\n",
    "    return torch.load(net_path)\n",
    "\n",
    "def get_fig(net,t):\n",
    "    # 图像数据转化\n",
    "    t = torch.unsqueeze(torch.tensor(data_transforms_infer(t)), dim = 0)\n",
    "    fig = net(t.to(torch.float32))\n",
    "    # ---- L2-norm Feature ------\n",
    "    ff = fig.data.cpu()\n",
    "    fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "    ff = ff.div(fnorm.expand_as(ff))\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387a339",
   "metadata": {},
   "source": [
    "### 目标人物特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aad4e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "net = get_reidnet('D:/Leon-Coding/Leon-PublicPlacesManage-System/models/reid-effi_b0.pth_net')\n",
    "fig_tar = get_fea(r'D:\\Leon-Coding\\Leon_TestData\\human-22.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b11c34",
   "metadata": {},
   "source": [
    "### 群体特征提取与比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592fa9a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "for key,val in yolo_mes.items():\n",
    "    crop_img = val['crop_img']\n",
    "    fig = get_fig(net,crop_img)\n",
    "    yolo_output['crop'][key]['fig'] = fig\n",
    "    yolo_output['crop'][key]['score'] = F.cosine_similarity(fig,fig_tar,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb06b0",
   "metadata": {},
   "source": [
    "### 获取最高相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13710765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_orig = np.array(yolo_output['img_orig'][0])\n",
    "print(type(img_orig))\n",
    "for key,val in yolo_mes.items():\n",
    "    crop_img = val['crop_img']\n",
    "    fig = get_fig(net,crop_img)\n",
    "    yolo_output['crop'][key]['fig'] = fig\n",
    "    yolo_output['crop'][key]['score'] = F.cosine_similarity(fig,fig_tar,dim=1)\n",
    "# 标注最高分\n",
    "score_list = []\n",
    "for key,val in yolo_mes.items():\n",
    "    score_list.append(yolo_output['crop'][key]['score'])\n",
    "\n",
    "key = str(score_list.index(max(score_list)))\n",
    "img_orig = draw_img(img_orig,yolo_output['crop'][key]['crop_bbox'],str(yolo_output['crop'][key]['score']))\n",
    "cv2.imwrite('c.jpg',img_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf92d6b0",
   "metadata": {},
   "source": [
    "## 行人属性识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0378f",
   "metadata": {},
   "source": [
    "### 基本函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef8acd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Leon-Coding\\Leon-PublicPlacesManage-System\\passer-attribute_get\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Oct  6 21:05:24 2022\n",
    "\n",
    "@author: leonz\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "if sys.platform == 'win32':\n",
    "    sys.path.append(r'D:\\Leon-Coding\\Leon_FC')\n",
    "else:\n",
    "    sys.path.append(r'/home/leonzion/Leon_Coding/Leon_FC')\n",
    "from leon_info import *\n",
    "from leon_os import *\n",
    "print(leon_path_in('passer-attribute_get'))\n",
    "from net import get_model\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "######################################################################\n",
    "# Argument\n",
    "# ---------\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--image_path', default=r'D:\\Leon-Coding\\Leon-PublicPlacesManage-System\\passer-attribute_get\\test_sample\\test_7.jpg',help='Path to test image')\n",
    "parser.add_argument('--dataset', default='market', type=str, help='dataset')\n",
    "parser.add_argument('--backbone', default='resnet50', type=str, help='model')\n",
    "parser.add_argument('--use-id', action='store_true', help='use identity loss')\n",
    "args = parser.parse_known_args()[0]\n",
    "assert args.dataset in ['market', 'duke']\n",
    "assert args.backbone in ['resnet50', 'resnet34', 'resnet18', 'densenet121']\n",
    "\n",
    "######################################################################\n",
    "# Settings\n",
    "# ---------\n",
    "dataset_dict = {\n",
    "    'market'  :  'Market-1501',\n",
    "    'duke'  :  'DukeMTMC-reID',\n",
    "}\n",
    "num_cls_dict = { 'market':30, 'duke':23 }\n",
    "num_ids_dict = { 'market':751, 'duke':702 }\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Model and Data\n",
    "# ---------\n",
    "def load_network(network):\n",
    "    save_path = os.path.join('models','passer_attribute.pth')\n",
    "    network.load_state_dict(torch.load(save_path))\n",
    "    print('Resume model from {}'.format(save_path))\n",
    "    return network\n",
    "\n",
    "def load_image(path):\n",
    "    src = Image.open(path)\n",
    "    src = transforms(src)\n",
    "    src = src.unsqueeze(dim=0)\n",
    "    return src\n",
    "\n",
    "\n",
    "def leon_get_attrmodel():\n",
    "######################################################################\n",
    "    model_name = '{}_nfc_id'.format(args.backbone) if args.use_id else '{}_nfc'.format(args.backbone)\n",
    "    num_label, num_id = num_cls_dict[args.dataset], num_ids_dict[args.dataset]\n",
    "    model = get_model(model_name, num_label, use_id=args.use_id, num_id=num_id)\n",
    "    model = load_network(model)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Inference\n",
    "# ---------\n",
    "class predict_decoder(object):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        with open(leon_path_in(os.path.join(r'passer-attribute_get','doc','label.json')), 'r') as f:\n",
    "            self.label_list = json.load(f)[dataset]\n",
    "        with open(leon_path_in(os.path.join(r'passer-attribute_get','doc','attribute.json')), 'r') as f:\n",
    "            self.attribute_dict = json.load(f)[dataset]\n",
    "        self.dataset = dataset\n",
    "        self.num_label = len(self.label_list)\n",
    "\n",
    "    def decode(self, pred):\n",
    "        pred = pred.squeeze(dim=0)\n",
    "        for idx in range(self.num_label):\n",
    "            name, chooce = self.attribute_dict[self.label_list[idx]]\n",
    "            if chooce[pred[idx]]:\n",
    "                print('{}: {}'.format(name, chooce[pred[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed43d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attri_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=(288, 144)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "def get_attribute(model,img):\n",
    "    src = attri_transforms(img).unsqueeze(dim=0)\n",
    "    src = model.forward(src)\n",
    "    pred = torch.gt(out, torch.ones_like(src)/2 )  # threshold=0.5\n",
    "    Dec = predict_decoder(args.dataset)\n",
    "    Dec.decode(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b8467dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume model from models\\passer_attribute.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4680\\2239652287.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_fig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcrop_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0myolo_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'crop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfig_tar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0myolo_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'crop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attri'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myolo_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'crop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'crop_img'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myolo_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'crop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attri'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4680\\2654508172.py\u001b[0m in \u001b[0;36mget_attribute\u001b[1;34m(model, img)\u001b[0m\n\u001b[0;32m      5\u001b[0m     ])\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattri_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m)\u001b[0m  \u001b[1;31m# threshold=0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"img should be PIL Image. Got {type(img)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "model = leon_get_model()\n",
    "for key,val in yolo_mes.items():\n",
    "    crop_img = val['crop_img']\n",
    "    fig = get_fig(net,crop_img)\n",
    "    yolo_output['crop'][key]['score'] = F.cosine_similarity(fig,fig_tar,dim=1)\n",
    "    yolo_output['crop'][key]['attri'] = get_attribute(model,yolo_output['crop'][key]['crop_img'])\n",
    "    print(yolo_output['crop'][key]['attri'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883dbc56",
   "metadata": {},
   "source": [
    "# 图像传输"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76801da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask, render_template, Response, make_response\n",
    "app = Flask(__name__)\n",
    " \n",
    "# 相机推流 - 创建对象 - 根据图生成二进制传输流\n",
    "def gen(camera):\n",
    "    while True:\n",
    "        frame = camera.get_frame()\n",
    "        # 生成二进制传输流 同时生成包含头部数据和图片数据的传输流\n",
    "        yield (b'--frame\\r\\n'\n",
    "               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n\\r\\n')\n",
    "\n",
    "# 每次访问，即返回一张处理好的照片 - 注意此处的response是用来进行数据的 反馈操作\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(gen(VideoCamera()),\n",
    "                    mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    " \n",
    "#当前实时相机画面\n",
    "@app.route('/cur_camera')\n",
    "def cur_camera():\n",
    "    return render_template(r'Flask-StreamData-Trans.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9783e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.241.8:5000\n",
      "Press CTRL+C to quit\n",
      "192.168.241.8 - - [06/Oct/2022 22:03:46] \"GET / HTTP/1.1\" 404 -\n",
      "192.168.241.8 - - [06/Oct/2022 22:03:46] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "192.168.241.8 - - [06/Oct/2022 22:04:00] \"GET /video_feed HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Leon_env",
   "language": "python",
   "name": "leon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
