{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6f72d0",
   "metadata": {},
   "source": [
    "# 实现 任务参数 记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fde146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30703fd5",
   "metadata": {},
   "source": [
    "# 图像源 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962188bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    " \n",
    "class VideoCamera(object):\n",
    "    def __init__(self):\n",
    "        # 定义视频数据的图像来源\n",
    "        self.video = cv2.VideoCapture(0)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.video.release()\n",
    "    \n",
    "    # 设置获取视频的图像数据\n",
    "    def get_frame(self):\n",
    "        # 每次运行都通过来源进行数据获取\n",
    "        success, image = self.video.read()\n",
    "        # 将图片进行编码，转化为指定的图片格式\n",
    "        ret, jpeg = cv2.imencode('.jpg', image)\n",
    "        # 通过字节形式进行流传输\n",
    "        return jpeg.tobytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8bf90",
   "metadata": {},
   "source": [
    "# 相关功能函数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ddc6b4",
   "metadata": {},
   "source": [
    "### 图像标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b8423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# cv2-图像 [x1,y1,x2,y2]-坐标框 贴图文本\n",
    "def draw_img(img,bbox,text):\n",
    "    image = img.copy()\n",
    "    # 左上角\n",
    "    start_point = (int(bbox[1]),int(bbox[0]))\n",
    "    # 右下角\n",
    "    end_point = (int(bbox[3]),int(bbox[2]))\n",
    "    # draw the rectangle\n",
    "    cv2.rectangle(image, start_point, end_point, (0, 0, 255), thickness= 3, lineType=cv2.LINE_8) \n",
    "    #org: Where you want to put the text\n",
    "    org = start_point\n",
    "    # write the text on the input image\n",
    "    cv2.putText(image, text, org, fontFace = cv2.FONT_HERSHEY_COMPLEX, fontScale = 1.8, color = (0,0,250))\n",
    "    # display the output\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a634f0",
   "metadata": {},
   "source": [
    "## 行人检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21eceb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tools/leon_yolo.pth model, and classes loaded.\n",
      "Configurations:\n",
      "----------------------------------------------------------------------\n",
      "|                     keys |                                   values|\n",
      "----------------------------------------------------------------------\n",
      "|               model_path |                      tools/leon_yolo.pth|\n",
      "|             classes_path |                            tools/cls.txt|\n",
      "|              input_shape |                               [960, 960]|\n",
      "|                      phi |                                Leon_nano|\n",
      "|               confidence |                                      0.5|\n",
      "|                  nms_iou |                                      0.3|\n",
      "|          letterbox_image |                                     True|\n",
      "|                     cuda |                                     True|\n",
      "----------------------------------------------------------------------\n",
      "\t- img value_type:PIL.JpegImagePlugin.JpegImageFile\n",
      "\t+ crop\n",
      "\t|   + 0\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 1\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 2\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 3\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 4\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 5\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 6\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 7\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 8\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 9\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 10\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 11\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 12\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 13\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 14\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 15\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 16\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 17\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 18\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 19\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 20\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 21\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 22\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t|   + 23\n",
      "\t|   |   - crop_img value_type:numpy.ndarray\n",
      "\t|   |   - crop_bbox value_type:numpy.ndarray\n",
      "\t|   |   - score value_type:numpy.ndarray\n",
      "\t- fea value_type:torch.Tensor\n",
      "\t- outputs value_type:list\n",
      "\t- img_orig value_type:list\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Oct  1 13:22:46 2022\n",
    "\n",
    "@author: leonzion\n",
    "\"\"\"\n",
    "\n",
    "from tools.yolo_leon_person import *\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from torchvision.ops import nms, roi_align, roi_pool\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "if sys.platform == 'win32':\n",
    "    sys.path.append(r'D:\\Leon-Coding\\Leon_FC')\n",
    "else:\n",
    "    sys.path.append(r'/home/leonzion/Leon_Coding/Leon_FC')\n",
    "from leon_info import *\n",
    "from leon_os import *\n",
    "# 初始化检测器\n",
    "yo = Yolo_infer_person()\n",
    "img = Image.open(r'D:\\Leon-Coding\\Leon_TestData\\mans_2.jpg')\n",
    "yolo_output = yo.infer(img)\n",
    "print(get_dict_dirs(yolo_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc5196",
   "metadata": {},
   "source": [
    "## 指定人物追踪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff83292",
   "metadata": {},
   "source": [
    "### 逐一获取行人数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e11ab6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yolo_mes = yolo_output['crop']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a852ec",
   "metadata": {},
   "source": [
    "### 重识别的函数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61d44a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Oct  2 12:58:46 2022\n",
    "\n",
    "@author: leonzion\n",
    "\"\"\"\n",
    "import sys\n",
    "if sys.platform == 'win32':\n",
    "    sys.path.append(r'D:\\Leon-Coding\\Leon_FC')\n",
    "else:\n",
    "    sys.path.append(r'/home/leonzion/Leon_Coding/Leon_FC')\n",
    "from leon_info import *\n",
    "from leon_os import *\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "leon_path_in(os.path.join('reid','Person_reID_baseline_pytorch-master','model','ft_ResNet50'))\n",
    "leon_path_in(os.path.join('reid','Person_reID_baseline_pytorch-master'))\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((256,128), interpolation=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "data_transforms_infer = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256,128), interpolation=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 基于图片获取特征\n",
    "def get_fea(path):\n",
    "    img = data_transforms(Image.open(path))\n",
    "#     img = torch.tensor(img.transpose(2, 0, 1))\n",
    "    img = torch.unsqueeze(img, dim = 0)\n",
    "    img = torch.tensor(img).to(torch.float32)\n",
    "    net = torch.load('D:/Leon-Coding/Leon-PublicPlacesManage-System/models/reid-effi_b0.pth_net')\n",
    "    outputs = net(img) \n",
    "    # ---- L2-norm Feature ------\n",
    "    ff = outputs.data.cpu()\n",
    "    fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "    ff = ff.div(fnorm.expand_as(ff))\n",
    "    return ff\n",
    "\n",
    "def get_reidnet(net_path):\n",
    "    # 'D:/Leon-Coding/Leon-PublicPlacesManage-System/models/reid-effi_b0.pth_net'\n",
    "    return torch.load(net_path)\n",
    "\n",
    "def get_fig(net,t):\n",
    "    # 图像数据转化\n",
    "    t = torch.unsqueeze(torch.tensor(data_transforms_infer(t)), dim = 0)\n",
    "    fig = net(t.to(torch.float32))\n",
    "    # ---- L2-norm Feature ------\n",
    "    ff = fig.data.cpu()\n",
    "    fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n",
    "    ff = ff.div(fnorm.expand_as(ff))\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59495169",
   "metadata": {},
   "source": [
    "### 目标人物特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aad4e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "net = get_reidnet('D:/Leon-Coding/Leon-PublicPlacesManage-System/models/reid-effi_b0.pth_net')\n",
    "fig_tar = get_fea(r'D:\\Leon-Coding\\Leon_TestData\\human-22.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7a9d9",
   "metadata": {},
   "source": [
    "### 群体特征提取与比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592fa9a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "for key,val in yolo_mes.items():\n",
    "    crop_img = val['crop_img']\n",
    "    fig = get_fig(net,crop_img)\n",
    "    yolo_output['crop'][key]['fig'] = fig\n",
    "    yolo_output['crop'][key]['score'] = F.cosine_similarity(fig,fig_tar,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d5f41",
   "metadata": {},
   "source": [
    "### 获取最高相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfe1afa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_orig = np.array(yolo_output['img_orig'][0])\n",
    "print(type(img_orig))\n",
    "for key,val in yolo_mes.items():\n",
    "    crop_img = val['crop_img']\n",
    "    fig = get_fig(net,crop_img)\n",
    "    yolo_output['crop'][key]['fig'] = fig\n",
    "    yolo_output['crop'][key]['score'] = F.cosine_similarity(fig,fig_tar,dim=1)\n",
    "# 标注最高分\n",
    "score_list = []\n",
    "for key,val in yolo_mes.items():\n",
    "    score_list.append(yolo_output['crop'][key]['score'])\n",
    "\n",
    "key = str(score_list.index(max(score_list)))\n",
    "img_orig = draw_img(img_orig,yolo_output['crop'][key]['crop_bbox'],str(yolo_output['crop'][key]['score']))\n",
    "cv2.imwrite('c.jpg',img_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5583ef",
   "metadata": {},
   "source": [
    "## 行人属性识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3564aa",
   "metadata": {},
   "source": [
    "### 基本函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91948f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Leon-Coding\\Leon-PublicPlacesManage-System\\passer-attribute_get\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Oct  6 21:05:24 2022\n",
    "\n",
    "@author: leonz\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "if sys.platform == 'win32':\n",
    "    sys.path.append(r'D:\\Leon-Coding\\Leon_FC')\n",
    "else:\n",
    "    sys.path.append(r'/home/leonzion/Leon_Coding/Leon_FC')\n",
    "from leon_info import *\n",
    "from leon_os import *\n",
    "print(leon_path_in('passer-attribute_get'))\n",
    "from net import get_model\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "######################################################################\n",
    "# Argument\n",
    "# ---------\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--image_path', default=r'D:\\Leon-Coding\\Leon-PublicPlacesManage-System\\passer-attribute_get\\test_sample\\test_7.jpg',help='Path to test image')\n",
    "parser.add_argument('--dataset', default='market', type=str, help='dataset')\n",
    "parser.add_argument('--backbone', default='resnet50', type=str, help='model')\n",
    "parser.add_argument('--use-id', action='store_true', help='use identity loss')\n",
    "args = parser.parse_known_args()[0]\n",
    "assert args.dataset in ['market', 'duke']\n",
    "assert args.backbone in ['resnet50', 'resnet34', 'resnet18', 'densenet121']\n",
    "\n",
    "######################################################################\n",
    "# Settings\n",
    "# ---------\n",
    "dataset_dict = {\n",
    "    'market'  :  'Market-1501',\n",
    "    'duke'  :  'DukeMTMC-reID',\n",
    "}\n",
    "num_cls_dict = { 'market':30, 'duke':23 }\n",
    "num_ids_dict = { 'market':751, 'duke':702 }\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Model and Data\n",
    "# ---------\n",
    "def load_network(network):\n",
    "    save_path = os.path.join('models','passer_attribute.pth')\n",
    "    network.load_state_dict(torch.load(save_path))\n",
    "    print('Resume model from {}'.format(save_path))\n",
    "    return network\n",
    "\n",
    "def load_image(path):\n",
    "    src = Image.open(path)\n",
    "    src = transforms(src)\n",
    "    src = src.unsqueeze(dim=0)\n",
    "    return src\n",
    "\n",
    "\n",
    "def leon_get_attrmodel():\n",
    "######################################################################\n",
    "    model_name = '{}_nfc_id'.format(args.backbone) if args.use_id else '{}_nfc'.format(args.backbone)\n",
    "    num_label, num_id = num_cls_dict[args.dataset], num_ids_dict[args.dataset]\n",
    "    model = get_model(model_name, num_label, use_id=args.use_id, num_id=num_id)\n",
    "    model = load_network(model)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Inference\n",
    "# ---------\n",
    "class predict_decoder(object):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        with open(leon_path_in(os.path.join(r'passer-attribute_get','doc','label.json')), 'r') as f:\n",
    "            self.label_list = json.load(f)[dataset]\n",
    "        with open(leon_path_in(os.path.join(r'passer-attribute_get','doc','attribute.json')), 'r') as f:\n",
    "            self.attribute_dict = json.load(f)[dataset]\n",
    "        self.dataset = dataset\n",
    "        self.num_label = len(self.label_list)\n",
    "\n",
    "    def decode(self, pred):\n",
    "        pred = pred.squeeze(dim=0)\n",
    "        for idx in range(self.num_label):\n",
    "            name, chooce = self.attribute_dict[self.label_list[idx]]\n",
    "            if chooce[pred[idx]]:\n",
    "                print('{}: {}'.format(name, chooce[pred[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f8bd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attri_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=(288, 144)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "def get_attribute(model,img):\n",
    "    src = attri_transforms(img).unsqueeze(dim=0)\n",
    "    src = model.forward(src)\n",
    "    pred = torch.gt(src, torch.ones_like(src)/2 )  # threshold=0.5\n",
    "    Dec = predict_decoder(args.dataset)\n",
    "    Dec.decode(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fce588",
   "metadata": {},
   "source": [
    "### 实际推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a26be1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume model from models\\passer_attribute.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leonz\\.conda\\envs\\Leon_Deep\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "          True, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: short\n",
      "sleeve length: short sleeve\n",
      "hair length: short hair\n",
      "wearing hat: no\n",
      "gender: male\n",
      "tensor([[False,  True, False, False, False, False, False,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: short hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of upper-body clothing: black\n",
      "color of lower-body clothing: blue\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "         False, False,  True,  True, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False,  True, False, False]])\n",
      "----\n",
      "age: adult\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: short hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of lower-body clothing: gray\n",
      "tensor([[False, False,  True, False, False, False, False,  True, False,  True,\n",
      "         False, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: male\n",
      "color of upper-body clothing: black\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "          True, False, False,  True, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "          True, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of upper-body clothing: black\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "          True, False,  True,  True, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: yes\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: short hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of upper-body clothing: black\n",
      "color of lower-body clothing: black\n",
      "tensor([[False,  True, False, False, False,  True, False,  True, False,  True,\n",
      "         False, False,  True,  True, False, False, False, False, False, False,\n",
      "         False,  True, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: yes\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: short\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of lower-body clothing: black\n",
      "tensor([[False,  True, False, False, False,  True, False,  True,  True,  True,\n",
      "          True, False,  True, False, False, False, False, False, False, False,\n",
      "         False,  True, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: yes\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of upper-body clothing: black\n",
      "color of lower-body clothing: gray\n",
      "tensor([[False,  True, False, False, False,  True, False,  True, False,  True,\n",
      "          True, False,  True,  True, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: short\n",
      "sleeve length: short sleeve\n",
      "hair length: short hair\n",
      "wearing hat: no\n",
      "gender: male\n",
      "color of upper-body clothing: black\n",
      "tensor([[False,  True, False, False, False, False, False,  True,  True,  True,\n",
      "         False, False, False,  True, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: yes\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of upper-body clothing: black\n",
      "tensor([[False,  True, False, False, False,  True, False,  True, False,  True,\n",
      "          True, False,  True,  True, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: short\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "tensor([[False,  True, False, False, False, False, False,  True,  True,  True,\n",
      "          True, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: short hair\n",
      "wearing hat: no\n",
      "gender: male\n",
      "color of upper-body clothing: black\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "         False, False, False,  True, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: long sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: male\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False, False,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of upper-body clothing: black\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "          True, False,  True,  True, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: short\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: male\n",
      "tensor([[False,  True, False, False, False, False, False,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: male\n",
      "color of upper-body clothing: green\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "          True, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: short hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "         False, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: yes\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: dress\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of lower-body clothing: gray\n",
      "tensor([[False,  True, False, False, False,  True, False, False, False,  True,\n",
      "          True, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "          True, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: long lower body clothing\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of lower-body clothing: gray\n",
      "tensor([[False,  True, False, False, False, False, False,  True, False,  True,\n",
      "          True, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True, False, False, False]])\n",
      "----\n",
      "age: teenager\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: short\n",
      "sleeve length: short sleeve\n",
      "hair length: long hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "tensor([[False,  True, False, False, False, False, False,  True,  True,  True,\n",
      "          True, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n",
      "carrying backpack: no\n",
      "carrying bag: no\n",
      "carrying handbag: no\n",
      "type of lower-body clothing: pants\n",
      "length of lower-body clothing: short\n",
      "sleeve length: short sleeve\n",
      "hair length: short hair\n",
      "wearing hat: no\n",
      "gender: female\n",
      "color of upper-body clothing: red\n",
      "tensor([[False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         False, False,  True, False, False,  True, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "model = leon_get_model()\n",
    "for key,val in yolo_mes.items():\n",
    "    crop_img = val['crop_img']\n",
    "    fig = get_fig(net,crop_img)\n",
    "    yolo_output['crop'][key]['attri'] = get_attribute(model,yolo_output['crop'][key]['crop_img'])\n",
    "    print(yolo_output['crop'][key]['attri'])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348375b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:[..]\n",
      "+--Leon-FaceSystem\n",
      "| +--a.jpg\n",
      "| +--encoding.py\n",
      "| +--face_dataset\n",
      "| | +--hg_0.jpg\n",
      "| | +--obama_1.jpg\n",
      "| | +--张学友_1.jpg\n",
      "| +--img\n",
      "| | +--obama.jpg\n",
      "| | +--zhangxueyou.jpg\n",
      "| +--leon-pre.py\n",
      "| +--LICENSE\n",
      "| +--model_data\n",
      "| | +--facenet_mobilenet.pth\n",
      "| | +--inception_resnetv1_face_encoding.npy\n",
      "| | +--inception_resnetv1_names.npy\n",
      "| | +--mobilenet_face_encoding.npy\n",
      "| | +--mobilenet_names.npy\n",
      "| | +--Retinaface_mobilenet0.25.pth\n",
      "| | +--simhei.ttf\n",
      "| +--nets\n",
      "| | +--facenet.py\n",
      "| | +--inception_resnetv1.py\n",
      "| | +--mobilenet.py\n",
      "| +--nets_retinaface\n",
      "| | +--layers.py\n",
      "| | +--mobilenet025.py\n",
      "| | +--retinaface.py\n",
      "| +--predict.py\n",
      "| +--README.md\n",
      "| +--requirements.txt\n",
      "| +--retinaface.py\n",
      "| +--utils\n",
      "| | +--anchors.py\n",
      "| | +--box_utils.py\n",
      "| | +--config.py\n",
      "| | +--utils.py\n",
      "| | +--utils_bbox.py\n",
      "| +--常见问题汇总.md\n",
      "| +--结构.md\n",
      "+--Leon-PublicPlacesManage-System\n",
      "| +--.gitattributes\n",
      "| +--.gitignore\n",
      "| +--.ipynb_checkpoints\n",
      "| | +--Untitled-checkpoint.ipynb\n",
      "| | +--对接与实现-checkpoint.ipynb\n",
      "| +--.spyproject\n",
      "| | +--config\n",
      "| | | +--backups\n",
      "| | | | +--codestyle.ini.bak\n",
      "| | | | +--encoding.ini.bak\n",
      "| | | | +--vcs.ini.bak\n",
      "| | | | +--workspace.ini.bak\n",
      "| | | +--codestyle.ini\n",
      "| | | +--defaults\n",
      "| | | | +--defaults-codestyle-0.2.0.ini\n",
      "| | | | +--defaults-encoding-0.2.0.ini\n",
      "| | | | +--defaults-vcs-0.2.0.ini\n",
      "| | | | +--defaults-workspace-0.2.0.ini\n",
      "| | | +--encoding.ini\n",
      "| | | +--vcs.ini\n",
      "| | | +--workspace.ini\n",
      "| +--get_tar.py\n",
      "| +--img_crop\n",
      "| +--jupyter\n",
      "| | +--.ipynb_checkpoints\n",
      "| | | +--Flask-Base-checkpoint.ipynb\n",
      "| | | +--Flask-StreamData-Trans-checkpoint.ipynb\n",
      "| | +--Flask-Base.ipynb\n",
      "| | +--Flask-StreamData-Trans.ipynb\n",
      "| | +--templates\n",
      "| | | +--Flask-StreamData-Trans.html\n",
      "| +--main.py\n",
      "| +--models\n",
      "| | +--passer_attribute.pth\n",
      "| | +--readme.md\n",
      "| | +--reid-effi_b0.pth\n",
      "| | +--reid-effi_b0.pth_net\n",
      "| | +--reid_orig.pth\n",
      "| | +--yolo_leon.pth\n",
      "| +--passer-attribute_get\n",
      "| | +--datafolder\n",
      "| | | +--folder.py\n",
      "| | | +--reid_dataset\n",
      "| | | | +--cuhk03_to_image.py\n",
      "| | | | +--gdrive_downloader.py\n",
      "| | | | +--import_CUHK01.py\n",
      "| | | | +--import_CUHK03.py\n",
      "| | | | +--import_DukeMTMC.py\n",
      "| | | | +--import_DukeMTMCAttribute.py\n",
      "| | | | +--import_Market1501.py\n",
      "| | | | +--import_Market1501Attribute.py\n",
      "| | | | +--import_MarketDuke.py\n",
      "| | | | +--import_MarketDuke_nodistractors.py\n",
      "| | | | +--import_VIPeR.py\n",
      "| | | | +--marketduke_to_hdf5.py\n",
      "| | | | +--pytorch_prepare.py\n",
      "| | | | +--reiddataset_downloader.py\n",
      "| | | | +--__init__.py\n",
      "| | +--doc\n",
      "| | | +--attribute.json\n",
      "| | | +--DukeMTMC-reID-attribute\n",
      "| | | +--label.json\n",
      "| | | +--Market1501-attribute\n",
      "| | | +--save_json.py\n",
      "| | +--inference.py\n",
      "| | +--net\n",
      "| | | +--models.py\n",
      "| | | +--utils.py\n",
      "| | | +--__init__.py\n",
      "| | +--README.md\n",
      "| | +--test.py\n",
      "| | +--test_sample\n",
      "| | | +--test_duke.jpg\n",
      "| | | +--test_market.jpg\n",
      "| | +--train.py\n",
      "| +--passer-attribute_get.py\n",
      "| +--reid\n",
      "| | +--leon-test.py\n",
      "| | +--Person_reID_baseline_pytorch-master\n",
      "| | | +--.github\n",
      "| | | | +--ISSUE_TEMPLATE\n",
      "| | | | | +--bug_report.md\n",
      "| | | | | +--feature_request.md\n",
      "| | | +--.gitignore\n",
      "| | | +--.travis.yml\n",
      "| | | +--circle_loss.py\n",
      "| | | +--colab\n",
      "| | | | +--README.md\n",
      "| | | +--demo.py\n",
      "| | | +--dgfolder.py\n",
      "| | | +--evaluate.py\n",
      "| | | +--evaluate_gpu.py\n",
      "| | | +--evaluate_rerank.py\n",
      "| | | +--GPU-Re-Ranking\n",
      "| | | | +--evaluate_rerank_gpu.py\n",
      "| | | | +--extension\n",
      "| | | | | +--adjacency_matrix\n",
      "| | | | | | +--build_adjacency_matrix.cpp\n",
      "| | | | | | +--build_adjacency_matrix_kernel.cu\n",
      "| | | | | | +--setup.py\n",
      "| | | | | +--make.sh\n",
      "| | | | | +--propagation\n",
      "| | | | | | +--gnn_propagate.cpp\n",
      "| | | | | | +--gnn_propagate_kernel.cu\n",
      "| | | | | | +--setup.py\n",
      "| | | | +--gnn_reranking.py\n",
      "| | | | +--README.md\n",
      "| | | | +--utils.py\n",
      "| | | +--instance_loss.py\n",
      "| | | +--leaderboard\n",
      "| | | | +--README.md\n",
      "| | | +--leaderboard-IR\n",
      "| | | | +--README.md\n",
      "| | | +--leon_infer\n",
      "| | | +--LICENSE\n",
      "| | | +--model\n",
      "| | | | +--.gitkeep\n",
      "| | | | +--ft_ResNet50\n",
      "| | | | | +--model.py\n",
      "| | | | | +--net_19.pth_net\n",
      "| | | | | +--opts.yaml\n",
      "| | | | | +--train.jpg\n",
      "| | | | | +--train.py\n",
      "| | | +--model.py\n",
      "| | | +--net_19.pth_net\n",
      "| | | +--ODFA.py\n",
      "| | | +--pdf\n",
      "| | | | +--.gitkeep\n",
      "| | | | +--3D-demo.png\n",
      "| | | | +--CVPR19_compressed.pdf\n",
      "| | | | +--ICCV17.pdf\n",
      "| | | | +--TMM20_compressed.pdf\n",
      "| | | +--prepare.py\n",
      "| | | +--prepare_CUB.py\n",
      "| | | +--prepare_MSMT.py\n",
      "| | | +--prepare_static.py\n",
      "| | | +--prepare_VehicleID.py\n",
      "| | | +--prepare_VeRi.py\n",
      "| | | +--prepare_viper.py\n",
      "| | | +--random_erasing.py\n",
      "| | | +--README.md\n",
      "| | | +--requirements.txt\n",
      "| | | +--re_ranking.py\n",
      "| | | +--show.png\n",
      "| | | +--sitemap.xml\n",
      "| | | +--test.py\n",
      "| | | +--test_with_TensorRT.py\n",
      "| | | +--tmp.py\n",
      "| | | +--tool\n",
      "| | | | +--clear_model.py\n",
      "| | | +--train.py\n",
      "| | | +--tutorial\n",
      "| | | | +--Answers_to_Quick_Questions.md\n",
      "| | | | +--README.md\n",
      "| | | +--utils.py\n",
      "| +--reid_get-figure.py\n",
      "| +--test.py\n",
      "| +--tools\n",
      "| | +--cls.txt\n",
      "| | +--fig_twice.py\n",
      "| | +--leon_yolo.pth\n",
      "| | +--simhei.ttf\n",
      "| | +--Yolo.py\n",
      "| | +--yolo_leon_person.py\n",
      "| +--Untitled.ipynb\n",
      "| +--yolov5_track\n",
      "| | +--.github\n",
      "| | | +--ISSUE_TEMPLATE\n",
      "| | | | +--bug.yml\n",
      "| | | | +--enhancement.yml\n",
      "| | | | +--question.yml\n",
      "| | | +--workflows\n",
      "| | | | +--ci-testing.yml\n",
      "| | | | +--stale.yml\n",
      "| | +--.gitignore\n",
      "| | +--.gitmodules\n",
      "| | +--cd\n",
      "| | +--conda\n",
      "| | +--LICENSE\n",
      "| | +--README.md\n",
      "| | +--reid_export.py\n",
      "| | +--requirements.txt\n",
      "| | +--runs\n",
      "| | | +--track\n",
      "| | | | +--exp\n",
      "| | | | +--exp10\n",
      "| | | | +--exp11\n",
      "| | | | +--exp12\n",
      "| | | | +--exp13\n",
      "| | | | +--exp14\n",
      "| | | | +--exp2\n",
      "| | | | +--exp3\n",
      "| | | | +--exp4\n",
      "| | | | +--exp5\n",
      "| | | | +--exp6\n",
      "| | | | +--exp7\n",
      "| | | | +--exp8\n",
      "| | | | +--exp9\n",
      "| | +--source\n",
      "| | +--strong_sort\n",
      "| | | +--.gitignore\n",
      "| | | +--configs\n",
      "| | | | +--strong_sort.yaml\n",
      "| | | +--deep\n",
      "| | | | +--checkpoint\n",
      "| | | | | +--.gitkeep\n",
      "| | | | | +--osnet_x0_25_market1501.pth\n",
      "| | | | | +--osnet_x0_25_msmt17.pth\n",
      "| | | | | +--osnet_x1_0_msmt17.pth\n",
      "| | | | +--reid\n",
      "| | | | +--reid_model_factory.py\n",
      "| | | | +--__init__.py\n",
      "| | | +--reid_multibackend.py\n",
      "| | | +--results\n",
      "| | | | +--output_04.gif\n",
      "| | | | +--output_th025.gif\n",
      "| | | | +--track_all_1280_025conf.gif\n",
      "| | | | +--track_pedestrians_1280_05conf.gif\n",
      "| | | +--sort\n",
      "| | | | +--detection.py\n",
      "| | | | +--iou_matching.py\n",
      "| | | | +--kalman_filter.py\n",
      "| | | | +--linear_assignment.py\n",
      "| | | | +--nn_matching.py\n",
      "| | | | +--preprocessing.py\n",
      "| | | | +--track.py\n",
      "| | | | +--tracker.py\n",
      "| | | | +--__init__.py\n",
      "| | | +--strong_sort.py\n",
      "| | | +--utils\n",
      "| | | | +--asserts.py\n",
      "| | | | +--draw.py\n",
      "| | | | +--evaluation.py\n",
      "| | | | +--io.py\n",
      "| | | | +--json_logger.py\n",
      "| | | | +--log.py\n",
      "| | | | +--parser.py\n",
      "| | | | +--tools.py\n",
      "| | | | +--__init__.py\n",
      "| | | +--__init__.py\n",
      "| | +--tar.mp4\n",
      "| | +--track.py\n",
      "| | +--val.py\n",
      "| | +--yolov5\n",
      "| | | +--.dockerignore\n",
      "| | | +--.gitattributes\n",
      "| | | +--.github\n",
      "| | | | +--CODE_OF_CONDUCT.md\n",
      "| | | | +--dependabot.yml\n",
      "| | | | +--ISSUE_TEMPLATE\n",
      "| | | | | +--bug-report.yml\n",
      "| | | | | +--config.yml\n",
      "| | | | | +--feature-request.yml\n",
      "| | | | | +--question.yml\n",
      "| | | | +--PULL_REQUEST_TEMPLATE.md\n",
      "| | | | +--README_cn.md\n",
      "| | | | +--SECURITY.md\n",
      "| | | | +--workflows\n",
      "| | | | | +--ci-testing.yml\n",
      "| | | | | +--codeql-analysis.yml\n",
      "| | | | | +--docker.yml\n",
      "| | | | | +--greetings.yml\n",
      "| | | | | +--rebase.yml\n",
      "| | | | | +--stale.yml\n",
      "| | | +--.gitignore\n",
      "| | | +--.pre-commit-config.yaml\n",
      "| | | +--classify\n",
      "| | | | +--predict.py\n",
      "| | | | +--train.py\n",
      "| | | | +--val.py\n",
      "| | | +--CONTRIBUTING.md\n",
      "| | | +--data\n",
      "| | | | +--Argoverse.yaml\n",
      "| | | | +--coco.yaml\n",
      "| | | | +--coco128.yaml\n",
      "| | | | +--GlobalWheat2020.yaml\n",
      "| | | | +--hyps\n",
      "| | | | | +--hyp.Objects365.yaml\n",
      "| | | | | +--hyp.scratch-high.yaml\n",
      "| | | | | +--hyp.scratch-low.yaml\n",
      "| | | | | +--hyp.scratch-med.yaml\n",
      "| | | | | +--hyp.VOC.yaml\n",
      "| | | | +--ImageNet.yaml\n",
      "| | | | +--images\n",
      "| | | | | +--bus.jpg\n",
      "| | | | | +--zidane.jpg\n",
      "| | | | +--Objects365.yaml\n",
      "| | | | +--scripts\n",
      "| | | | | +--download_weights.sh\n",
      "| | | | | +--get_coco.sh\n",
      "| | | | | +--get_coco128.sh\n",
      "| | | | | +--get_imagenet.sh\n",
      "| | | | +--SKU-110K.yaml\n",
      "| | | | +--VisDrone.yaml\n",
      "| | | | +--VOC.yaml\n",
      "| | | | +--xView.yaml\n",
      "| | | +--detect.py\n",
      "| | | +--export.py\n",
      "| | | +--hubconf.py\n",
      "| | | +--LICENSE\n",
      "| | | +--models\n",
      "| | | | +--common.py\n",
      "| | | | +--experimental.py\n",
      "| | | | +--hub\n",
      "| | | | | +--anchors.yaml\n",
      "| | | | | +--yolov3-spp.yaml\n",
      "| | | | | +--yolov3-tiny.yaml\n",
      "| | | | | +--yolov3.yaml\n",
      "| | | | | +--yolov5-bifpn.yaml\n",
      "| | | | | +--yolov5-fpn.yaml\n",
      "| | | | | +--yolov5-p2.yaml\n",
      "| | | | | +--yolov5-p34.yaml\n",
      "| | | | | +--yolov5-p6.yaml\n",
      "| | | | | +--yolov5-p7.yaml\n",
      "| | | | | +--yolov5-panet.yaml\n",
      "| | | | | +--yolov5l6.yaml\n",
      "| | | | | +--yolov5m6.yaml\n",
      "| | | | | +--yolov5n6.yaml\n",
      "| | | | | +--yolov5s-ghost.yaml\n",
      "| | | | | +--yolov5s-transformer.yaml\n",
      "| | | | | +--yolov5s6.yaml\n",
      "| | | | | +--yolov5x6.yaml\n",
      "| | | | +--tf.py\n",
      "| | | | +--yolo.py\n",
      "| | | | +--yolov5l.yaml\n",
      "| | | | +--yolov5m.yaml\n",
      "| | | | +--yolov5n.yaml\n",
      "| | | | +--yolov5s.yaml\n",
      "| | | | +--yolov5x.yaml\n",
      "| | | | +--__init__.py\n",
      "| | | +--README.md\n",
      "| | | +--requirements.txt\n",
      "| | | +--setup.cfg\n",
      "| | | +--tmp.py\n",
      "| | | +--train.py\n",
      "| | | +--tutorial.ipynb\n",
      "| | | +--utils\n",
      "| | | | +--activations.py\n",
      "| | | | +--augmentations.py\n",
      "| | | | +--autoanchor.py\n",
      "| | | | +--autobatch.py\n",
      "| | | | +--aws\n",
      "| | | | | +--mime.sh\n",
      "| | | | | +--resume.py\n",
      "| | | | | +--userdata.sh\n",
      "| | | | | +--__init__.py\n",
      "| | | | +--benchmarks.py\n",
      "| | | | +--callbacks.py\n",
      "| | | | +--dataloaders.py\n",
      "| | | | +--docker\n",
      "| | | | | +--Dockerfile\n",
      "| | | | | +--Dockerfile-arm64\n",
      "| | | | | +--Dockerfile-cpu\n",
      "| | | | +--downloads.py\n",
      "| | | | +--flask_rest_api\n",
      "| | | | | +--example_request.py\n",
      "| | | | | +--README.md\n",
      "| | | | | +--restapi.py\n",
      "| | | | +--general.py\n",
      "| | | | +--google_app_engine\n",
      "| | | | | +--additional_requirements.txt\n",
      "| | | | | +--app.yaml\n",
      "| | | | | +--Dockerfile\n",
      "| | | | +--loggers\n",
      "| | | | | +--clearml\n",
      "| | | | | | +--clearml_utils.py\n",
      "| | | | | | +--hpo.py\n",
      "| | | | | | +--README.md\n",
      "| | | | | | +--__init__.py\n",
      "| | | | | +--wandb\n",
      "| | | | | | +--log_dataset.py\n",
      "| | | | | | +--README.md\n",
      "| | | | | | +--sweep.py\n",
      "| | | | | | +--sweep.yaml\n",
      "| | | | | | +--wandb_utils.py\n",
      "| | | | | | +--__init__.py\n",
      "| | | | | +--__init__.py\n",
      "| | | | +--loss.py\n",
      "| | | | +--metrics.py\n",
      "| | | | +--plots.py\n",
      "| | | | +--torch_utils.py\n",
      "| | | | +--__init__.py\n",
      "| | | +--val.py\n",
      "| | | +--weights\n",
      "| | | | +--crowdhuman_yolov5m.pt\n",
      "| +--yolo_infer.py\n",
      "| +--yolo_infer_person.py\n",
      "| +--zip\n",
      "| | +--facenet-retinaface-pytorch-main.zip\n",
      "| | +--Person-Attribute-Recognition-MarketDuke-master.zip\n",
      "| | +--Person_reID_baseline_pytorch-master.zip\n",
      "| | +--Yolov5_StrongSORT_OSNet-master.zip\n",
      "| +--后端接口思路.pdf\n",
      "| +--对接与实现.ipynb\n",
      "| +--项目复现.md\n",
      "| +--项目实现.md\n",
      "| +--项目结构.md\n",
      "| +--项目规划.md\n",
      "+--Leon_FC\n",
      "| +--.gitattributes\n",
      "| +--leon_image.py\n",
      "| +--leon_info.py\n",
      "| +--leon_os.py\n",
      "| +--leon_testdata.py\n",
      "| +--leon_tools.py\n",
      "| +--setup.py\n",
      "| +--__init__.py\n",
      "+--Leon_TestData\n",
      "| +--.gitattributes\n",
      "| +--cars.mp4\n",
      "| +--clipchamap.ico\n",
      "| +--hg-0.png\n",
      "| +--hg-1.png\n",
      "| +--human-11.png\n",
      "| +--human-12.png\n",
      "| +--human-13.jpg\n",
      "| +--human-13.png\n",
      "| +--human-15.png\n",
      "| +--human-21.png\n",
      "| +--human-22.png\n",
      "| +--human-5.png\n",
      "| +--human-6.jpg\n",
      "| +--human-7.jpg\n",
      "| +--human-8.jpg\n",
      "| +--human-9.jpg\n",
      "| +--human_0.jpg\n",
      "| +--human_1.jpg\n",
      "| +--human_2.jpg\n",
      "| +--lyf-0.png\n",
      "| +--lyf-1.png\n",
      "| +--mans_0.jpg\n",
      "| +--mans_0.mp4\n",
      "| +--mans_1.jpg\n",
      "| +--mans_2.jpg\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "883dbc56",
   "metadata": {},
   "source": [
    "# 图像传输"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76801da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask, render_template, Response, make_response\n",
    "app = Flask(__name__)\n",
    " \n",
    "# 相机推流 - 创建对象 - 根据图生成二进制传输流\n",
    "def gen(camera):\n",
    "    while True:\n",
    "        frame = camera.get_frame()\n",
    "        # 生成二进制传输流 同时生成包含头部数据和图片数据的传输流\n",
    "        yield (b'--frame\\r\\n'\n",
    "               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n\\r\\n')\n",
    "\n",
    "# 每次访问，即返回一张处理好的照片 - 注意此处的response是用来进行数据的 反馈操作\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(gen(VideoCamera()),\n",
    "                    mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    " \n",
    "#当前实时相机画面\n",
    "@app.route('/cur_camera')\n",
    "def cur_camera():\n",
    "    return render_template(r'Flask-StreamData-Trans.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9783e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.241.8:5000\n",
      "Press CTRL+C to quit\n",
      "192.168.241.8 - - [06/Oct/2022 22:03:46] \"GET / HTTP/1.1\" 404 -\n",
      "192.168.241.8 - - [06/Oct/2022 22:03:46] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "192.168.241.8 - - [06/Oct/2022 22:04:00] \"GET /video_feed HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Leon_env",
   "language": "python",
   "name": "leon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
